{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Library"
      ],
      "metadata": {
        "id": "WlaqVSa4zYOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "iGN4PnHJzSvS",
        "outputId": "1166eadf-9efe-4952-b773-5098d7ca6ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# !pip install sklearn_crfsuite\n",
        "!pip install -U numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "nnZXqWdcziOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pprint, time\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "# from sklearn_crfsuite import CRF\n",
        "# from sklearn_crfsuite import metrics\n",
        "# from sklearn_crfsuite import scorers\n",
        "from collections import Counter\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "C8lTsO3ozhUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "F9x2jcSo0jlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"treebank\")\n",
        "nltk.download(\"universal_tagset\")"
      ],
      "metadata": {
        "id": "Qr2Hn_h_0Sbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f7249d-b68b-44a3-de1b-82995e2b8790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset=\"universal\")"
      ],
      "metadata": {
        "id": "XMGRxuVq03TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Tagged Sentences\",len(tagged_sentence))\n",
        "tagged_words = [tup for sent in tagged_sentence for tup in sent]\n",
        "print(\"Total Number of Tagged words\", len(tagged_words))\n",
        "vocab = set([word for word, tag in tagged_words])\n",
        "print(\"Vocabulary of the Corpus\",len(vocab))\n",
        "tags = set([tag for word,tag in tagged_words])\n",
        "print(\"Number of Tags in the Corpus\",len(tags))\n"
      ],
      "metadata": {
        "id": "OOzig5LI0mUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12d00e7-04d1-4c9d-ad53-31c445fb936b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tagged Sentences 3914\n",
            "Total Number of Tagged words 100676\n",
            "Vocabulary of the Corpus 12408\n",
            "Number of Tags in the Corpus 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = train_test_split(tagged_sentence, test_size=0.2, random_state=1234)\n",
        "print(\"Number of Sentences in Training Data\",len(train_set))\n",
        "print(\"Number of Sentences in Test Data\",len(test_set))\n"
      ],
      "metadata": {
        "id": "ua_V4WiN11l7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddbcde7-567c-401b-9211-c35f5cf72dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sentences in Training Data 3131\n",
            "Number of Sentences in Test Data 783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Leaning"
      ],
      "metadata": {
        "id": "fsz8jQ2L5TdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the feature function. The following features can be used:\n",
        "1. Is the first letter capitalized?\n",
        "2. Is it the first word in the sentence?\n",
        "3. Is it the last word?\n",
        "4. What is the prefix of the word?\n",
        "5. What is the suffix of the word?\n",
        "6. Is the complete word capitalized\n",
        "7. What is the previous word?\n",
        "8. What is the next word?\n",
        "9. Is it numeric?\n",
        "10. Is it alphaunumeric?\n",
        "11. Is there a hyphen in the word?"
      ],
      "metadata": {
        "id": "QeCw_j-I3hS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# work feature\n",
        "def features(sentence,index):\n",
        "  return {\n",
        "      'is_first_capital':int(sentence[index][0].isupper()),\n",
        "      'is_first_word':int(index==0),\n",
        "      'is_last_word':int(index==len(sentence)-1),\n",
        "      'is_complete_capital':int(sentence[index].upper()==sentence[index]),\n",
        "      'prev_word':'' if index==0 else sentence[index-1],\n",
        "      'next_word':'' if index==len(sentence)-1 else sentence[index+1],\n",
        "      'is_numeric':int(sentence[index].isdigit()),\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-z])',sentence[index])))),\n",
        "      'prefix_1':sentence[index][0],\n",
        "      'prefix_2':sentence[index][:2],\n",
        "      'prefix_3':sentence[index][:3],\n",
        "      'prefix_4':sentence[index][:4],\n",
        "      'suffix_1':sentence[index][-1],\n",
        "      'suffix_2':sentence[index][-2:],\n",
        "      'suffix_3':sentence[index][-3:],\n",
        "      'suffix_4':sentence[index][-4:],\n",
        "      'word_has_hypen': 1 if '-' in sentence[index] else 0\n",
        "  }\n",
        "\n",
        "def untag(sentence):\n",
        "  return [word for word,tag in sentence]\n",
        "# prepare data\n",
        "def prepareData(tagged_sentences):\n",
        "  X,y=[],[]\n",
        "  for sentences in tagged_sentences:\n",
        "    X.append([features(untag(sentences), index) for index in range(len(sentences))])\n",
        "    y.append([tag for word, tag in sentences])\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "-5aa9oWL46LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train=prepareData(train_set)\n",
        "X_test,y_test=prepareData(test_set)"
      ],
      "metadata": {
        "id": "xFkBdc-978oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only on sequence\n",
        "X_train[0]"
      ],
      "metadata": {
        "id": "mY4EmbBp72up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dd3de1-a38e-48c7-cbad-0b0d832ed9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'is_first_capital': 1,\n",
              "  'is_first_word': 1,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': '',\n",
              "  'next_word': 'Wall',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'O',\n",
              "  'prefix_2': 'On',\n",
              "  'prefix_3': 'On',\n",
              "  'prefix_4': 'On',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'On',\n",
              "  'suffix_3': 'On',\n",
              "  'suffix_4': 'On',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 1,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'On',\n",
              "  'next_word': 'Street',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'W',\n",
              "  'prefix_2': 'Wa',\n",
              "  'prefix_3': 'Wal',\n",
              "  'prefix_4': 'Wall',\n",
              "  'suffix_1': 'l',\n",
              "  'suffix_2': 'll',\n",
              "  'suffix_3': 'all',\n",
              "  'suffix_4': 'Wall',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 1,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'Wall',\n",
              "  'next_word': 'men',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'S',\n",
              "  'prefix_2': 'St',\n",
              "  'prefix_3': 'Str',\n",
              "  'prefix_4': 'Stre',\n",
              "  'suffix_1': 't',\n",
              "  'suffix_2': 'et',\n",
              "  'suffix_3': 'eet',\n",
              "  'suffix_4': 'reet',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'Street',\n",
              "  'next_word': 'and',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'm',\n",
              "  'prefix_2': 'me',\n",
              "  'prefix_3': 'men',\n",
              "  'prefix_4': 'men',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'men',\n",
              "  'suffix_4': 'men',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'men',\n",
              "  'next_word': 'women',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'a',\n",
              "  'prefix_2': 'an',\n",
              "  'prefix_3': 'and',\n",
              "  'prefix_4': 'and',\n",
              "  'suffix_1': 'd',\n",
              "  'suffix_2': 'nd',\n",
              "  'suffix_3': 'and',\n",
              "  'suffix_4': 'and',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'and',\n",
              "  'next_word': 'walk',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wo',\n",
              "  'prefix_3': 'wom',\n",
              "  'prefix_4': 'wome',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'men',\n",
              "  'suffix_4': 'omen',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'women',\n",
              "  'next_word': 'with',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wa',\n",
              "  'prefix_3': 'wal',\n",
              "  'prefix_4': 'walk',\n",
              "  'suffix_1': 'k',\n",
              "  'suffix_2': 'lk',\n",
              "  'suffix_3': 'alk',\n",
              "  'suffix_4': 'walk',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'walk',\n",
              "  'next_word': 'great',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wi',\n",
              "  'prefix_3': 'wit',\n",
              "  'prefix_4': 'with',\n",
              "  'suffix_1': 'h',\n",
              "  'suffix_2': 'th',\n",
              "  'suffix_3': 'ith',\n",
              "  'suffix_4': 'with',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'with',\n",
              "  'next_word': 'purpose',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'g',\n",
              "  'prefix_2': 'gr',\n",
              "  'prefix_3': 'gre',\n",
              "  'prefix_4': 'grea',\n",
              "  'suffix_1': 't',\n",
              "  'suffix_2': 'at',\n",
              "  'suffix_3': 'eat',\n",
              "  'suffix_4': 'reat',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'great',\n",
              "  'next_word': ',',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'p',\n",
              "  'prefix_2': 'pu',\n",
              "  'prefix_3': 'pur',\n",
              "  'prefix_4': 'purp',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'se',\n",
              "  'suffix_3': 'ose',\n",
              "  'suffix_4': 'pose',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'prev_word': 'purpose',\n",
              "  'next_word': '*-2',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': ',',\n",
              "  'prefix_2': ',',\n",
              "  'prefix_3': ',',\n",
              "  'prefix_4': ',',\n",
              "  'suffix_1': ',',\n",
              "  'suffix_2': ',',\n",
              "  'suffix_3': ',',\n",
              "  'suffix_4': ',',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'prev_word': ',',\n",
              "  'next_word': 'noticing',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': '*',\n",
              "  'prefix_2': '*-',\n",
              "  'prefix_3': '*-2',\n",
              "  'prefix_4': '*-2',\n",
              "  'suffix_1': '2',\n",
              "  'suffix_2': '-2',\n",
              "  'suffix_3': '*-2',\n",
              "  'suffix_4': '*-2',\n",
              "  'word_has_hypen': 1},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': '*-2',\n",
              "  'next_word': 'one',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'n',\n",
              "  'prefix_2': 'no',\n",
              "  'prefix_3': 'not',\n",
              "  'prefix_4': 'noti',\n",
              "  'suffix_1': 'g',\n",
              "  'suffix_2': 'ng',\n",
              "  'suffix_3': 'ing',\n",
              "  'suffix_4': 'cing',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'noticing',\n",
              "  'next_word': 'another',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'o',\n",
              "  'prefix_2': 'on',\n",
              "  'prefix_3': 'one',\n",
              "  'prefix_4': 'one',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'ne',\n",
              "  'suffix_3': 'one',\n",
              "  'suffix_4': 'one',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'one',\n",
              "  'next_word': 'only',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'a',\n",
              "  'prefix_2': 'an',\n",
              "  'prefix_3': 'ano',\n",
              "  'prefix_4': 'anot',\n",
              "  'suffix_1': 'r',\n",
              "  'suffix_2': 'er',\n",
              "  'suffix_3': 'her',\n",
              "  'suffix_4': 'ther',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'another',\n",
              "  'next_word': 'when',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'o',\n",
              "  'prefix_2': 'on',\n",
              "  'prefix_3': 'onl',\n",
              "  'prefix_4': 'only',\n",
              "  'suffix_1': 'y',\n",
              "  'suffix_2': 'ly',\n",
              "  'suffix_3': 'nly',\n",
              "  'suffix_4': 'only',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'only',\n",
              "  'next_word': 'they',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wh',\n",
              "  'prefix_3': 'whe',\n",
              "  'prefix_4': 'when',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'hen',\n",
              "  'suffix_4': 'when',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'when',\n",
              "  'next_word': 'jostle',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 't',\n",
              "  'prefix_2': 'th',\n",
              "  'prefix_3': 'the',\n",
              "  'prefix_4': 'they',\n",
              "  'suffix_1': 'y',\n",
              "  'suffix_2': 'ey',\n",
              "  'suffix_3': 'hey',\n",
              "  'suffix_4': 'they',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'they',\n",
              "  'next_word': 'for',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'j',\n",
              "  'prefix_2': 'jo',\n",
              "  'prefix_3': 'jos',\n",
              "  'prefix_4': 'jost',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'le',\n",
              "  'suffix_3': 'tle',\n",
              "  'suffix_4': 'stle',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'jostle',\n",
              "  'next_word': 'cabs',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'f',\n",
              "  'prefix_2': 'fo',\n",
              "  'prefix_3': 'for',\n",
              "  'prefix_4': 'for',\n",
              "  'suffix_1': 'r',\n",
              "  'suffix_2': 'or',\n",
              "  'suffix_3': 'for',\n",
              "  'suffix_4': 'for',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'prev_word': 'for',\n",
              "  'next_word': '*T*-1',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': 'c',\n",
              "  'prefix_2': 'ca',\n",
              "  'prefix_3': 'cab',\n",
              "  'prefix_4': 'cabs',\n",
              "  'suffix_1': 's',\n",
              "  'suffix_2': 'bs',\n",
              "  'suffix_3': 'abs',\n",
              "  'suffix_4': 'cabs',\n",
              "  'word_has_hypen': 0},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'prev_word': 'cabs',\n",
              "  'next_word': '.',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 1,\n",
              "  'prefix_1': '*',\n",
              "  'prefix_2': '*T',\n",
              "  'prefix_3': '*T*',\n",
              "  'prefix_4': '*T*-',\n",
              "  'suffix_1': '1',\n",
              "  'suffix_2': '-1',\n",
              "  'suffix_3': '*-1',\n",
              "  'suffix_4': 'T*-1',\n",
              "  'word_has_hypen': 1},\n",
              " {'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 1,\n",
              "  'is_complete_capital': 1,\n",
              "  'prev_word': '*T*-1',\n",
              "  'next_word': '',\n",
              "  'is_numeric': 0,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix_1': '.',\n",
              "  'prefix_2': '.',\n",
              "  'prefix_3': '.',\n",
              "  'prefix_4': '.',\n",
              "  'suffix_1': '.',\n",
              "  'suffix_2': '.',\n",
              "  'suffix_3': '.',\n",
              "  'suffix_4': '.',\n",
              "  'word_has_hypen': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "v0jnZaUf9iMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07f4e2c-8028-4b6c-de5c-35c4e77d9cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ADP',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'CONJ',\n",
              " 'NOUN',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " 'ADJ',\n",
              " 'NOUN',\n",
              " '.',\n",
              " 'X',\n",
              " 'VERB',\n",
              " 'NUM',\n",
              " 'DET',\n",
              " 'ADV',\n",
              " 'ADV',\n",
              " 'PRON',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " 'NOUN',\n",
              " 'X',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model with CRF"
      ],
      "metadata": {
        "id": "M5Bf6nkj90Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = CRF(\n",
        "    algorithm=\"lbfgs\",\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "# crf.fit(X_train, y_train)\n",
        "\n",
        "try:\n",
        "    crf.fit(X_train, y_train)\n",
        "except AttributeError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "keF4CDpJ9wdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=crf.predict(X_test)"
      ],
      "metadata": {
        "id": "77bhC0H1-O6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=crf.classes_)"
      ],
      "metadata": {
        "id": "GeApuz0Q-lM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0617e41-d92f-49a0-fb92-33d73a4d6e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9740622075583344"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train=crf.predict(X_train)\n",
        "metrics.flat_f1_score(y_train, y_pred_train, average='weighted', labels=crf.classes_)"
      ],
      "metadata": {
        "id": "qzL4_l-F-_s9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc7baf8-75f0-4db3-cce4-c6ef9b8436ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9962782771443118"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "9KCBbtP3eG0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dataset"
      ],
      "metadata": {
        "id": "9r6jdwHAeV21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()[:100]\n",
        "\n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \",len(tagged_sentences))\n",
        "print(\"Tagged words:\",len(nltk.corpus.treebank.tagged_words()))"
      ],
      "metadata": {
        "id": "aAgYD71CAGPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba55f39-6697-4c88-e0ba-42cddefe239c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  100\n",
            "Tagged words: 100676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sentences, sentence_tags = [], []\n",
        "for tagged_sentence in tagged_sentences:\n",
        "  sentence, tags = zip(*tagged_sentence)\n",
        "  sentences.append(np.array(sentence))\n",
        "  sentence_tags.append(np.array(tags))\n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CIQ_2use87N",
        "outputId": "05d3698e-2f80-447b-9b7a-d67fac7a8397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train_test_split"
      ],
      "metadata": {
        "id": "XhgW7sfHgrie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_sentences,\n",
        " test_sentences,\n",
        " train_tags,\n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n",
        "print(len(train_sentences),len(test_sentences),len(train_tags),len(test_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUqD4tK0gPQr",
        "outputId": "d6d72f28-fcd0-4096-e461-0b07620ff9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 20 80 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transfer character to number for train in deep learning"
      ],
      "metadata": {
        "id": "lmYDRF7ohmaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words, tags = set([]), set([])\n",
        "\n",
        "for s in train_sentences:\n",
        "  for w in s:\n",
        "    words.add(w.lower())\n",
        "\n",
        "  for ts in train_tags:\n",
        "    for t in ts:\n",
        "      tags.add(t)\n",
        "\n",
        "  word2index = {w: i + 2 for i, w in enumerate(list(words))} # start in index 2 because start padding index 1 and OOVs index 2\n",
        "  word2index['-PAD-'] = 0 # The special value used for padding to add 0 for nan values caused by unequal sentences (long sentences do not add 0, short sentences produce nan values, must add 0)\n",
        "  word2index['-OOV-'] = 1 # The special value used for OOVs(out of vocab) Words that are not in our dataset are new words.\n",
        "\n",
        "  tag2index = {t: i + 1 for i, t in enumerate(list(tags))} # start in index 1 bacause start padding index = 0\n",
        "  tag2index['-PAD-'] = 0 # same case in sentece add 0 for nan values caused by unequal sentences (long sentences do not add 0, short sentences produce nan values, must add 0)"
      ],
      "metadata": {
        "id": "zGph2cVUhRyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tag2index)\n",
        "test_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkRryWjc9rkT",
        "outputId": "2022376d-f223-41b2-9eb3-58a3a40fa21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'WDT', '-NONE-', 'VBD', 'NN',\n",
              "        'IN', 'DT', 'NNP', 'NNS', 'VBD', 'VBN', '-NONE-', 'TO', 'NN', 'IN',\n",
              "        'DT', 'CD', '.'], dtype='<U6'),\n",
              " array(['WRB', 'PRP', 'VBZ', 'NN', 'IN', 'PRP$', 'JJ', 'NN', '-NONE-', ',',\n",
              "        'DT', 'NN', 'POS', 'VBG', 'NNS', 'RB', 'VBP', 'RP', 'TO', 'DT',\n",
              "        'JJ', 'NNS', 'IN', 'NN', 'NNS', 'IN', 'NNP', 'NNP', 'CC', 'NNP',\n",
              "        'NNP', '.'], dtype='<U6'),\n",
              " array(['PRP', 'VBZ', 'CD', 'NNS', 'CC', 'VBZ', 'JJ', 'NN', 'IN', 'IN',\n",
              "        '$', 'CD', 'CD', '-NONE-', '.'], dtype='<U6'),\n",
              " array(['NN', 'NNS', 'VBD', '-NONE-', 'NNS', 'IN', 'DT', 'NN', 'IN', 'DT',\n",
              "        'NN', 'MD', 'VB', 'IN', 'DT', 'NN', 'NN', 'IN', '$', 'CD', 'CD',\n",
              "        '-NONE-', '.'], dtype='<U6'),\n",
              " array(['IN', 'NNP', ',', 'DT', 'NNP', 'NNP', 'NNP', 'VBD', 'DT', 'JJ',\n",
              "        'NN', 'IN', 'RB', 'DT', 'NNS', 'IN', 'NN', '.'], dtype='<U3'),\n",
              " array(['IN', 'CD', 'NNS', 'WP', '-NONE-', 'VBD', 'RB', 'IN', 'DT', 'NN',\n",
              "        ',', 'CD', '-NONE-', 'VBP', 'VBN', ':', 'JJ', 'IN', 'CD', 'NNS',\n",
              "        'DT', 'VBN', 'NN', '.'], dtype='<U6'),\n",
              " array(['NNP', 'NNP', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', 'DT', 'NNP',\n",
              "        'NNP', 'NNP', 'CC', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'NNP', 'CC',\n",
              "        'NNP', 'NNP', '.'], dtype='<U3'),\n",
              " array(['PRP', 'VBZ', 'NNP', 'NNP', 'NNP', ',', 'RB', 'DT', 'NNP', 'NNP',\n",
              "        'NN', 'NN', ',', 'WP', '-NONE-', 'VBD', '.'], dtype='<U6'),\n",
              " array(['PRP', 'VBZ', 'RB', 'IN', 'JJ', 'NNS', 'RB', 'CC', 'VBZ', 'RB',\n",
              "        'VBG', 'NN', 'NNS', ',', 'WDT', '-NONE-', 'VBZ', 'PRP$', 'NN', '.'],\n",
              "       dtype='<U6'),\n",
              " array(['NNS', 'IN', 'JJ', 'JJ', 'NNS', 'VBD', '-NONE-', 'TO', 'VB', ',',\n",
              "        'IN', 'NNS', 'IN', 'NN', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'NN',\n",
              "        'NNS', '.'], dtype='<U6'),\n",
              " array(['NNS', 'IN', 'DT', 'CD', 'JJ', 'NNS', 'VBD', 'IN', '$', 'CD', 'CD',\n",
              "        '-NONE-', 'IN', 'DT', 'JJS', 'NN', ',', 'TO', '$', 'CD', 'CD',\n",
              "        '-NONE-', '.'], dtype='<U6'),\n",
              " array(['DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'CD', 'JJ', 'NNS', 'VBN',\n",
              "        '-NONE-', 'IN', 'NNP', 'POS', 'NNP', 'NNP', 'NNP', 'VBD', 'DT',\n",
              "        'NN', 'IN', 'DT', 'NN', 'NN', 'TO', 'CD', 'NN', 'IN', 'CD', 'NN',\n",
              "        'IN', 'DT', 'NN', 'VBD', 'NNP', '.'], dtype='<U6'),\n",
              " array(['NNP', 'NNP', 'NNP', 'VBD', 'VBN', '-NONE-', 'JJ', 'NN', 'NN',\n",
              "        '-NONE-', 'CC', 'JJ', 'NN', '-NONE-', 'IN', 'DT', 'NNP', 'NNS',\n",
              "        'CC', 'NN', 'NN', 'IN', 'JJ', 'NN', 'NN', 'NNP', 'NNP', 'NNP', '.'],\n",
              "       dtype='<U6'),\n",
              " array(['DT', 'NNP', 'NN', 'VBD', ',', '``', 'DT', 'VBZ', 'DT', 'JJ', 'NN',\n",
              "        '.'], dtype='<U3'),\n",
              " array(['PRP', 'VBP', 'VBG', 'IN', 'NNS', 'IN', 'IN', 'NN', 'VBD', 'IN',\n",
              "        'NN', 'VBG', 'DT', 'JJ', 'NNS', '.'], dtype='<U3'),\n",
              " array(['DT', 'NN', 'RB', 'MD', 'VB', 'DT', 'WP', '-NONE-', 'VBP', 'IN',\n",
              "        'DT', 'NNP', 'MD', 'VB', 'DT', 'NN', 'IN', 'NN', 'VBG', 'NN',\n",
              "        'RBR', 'RB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', ',', 'NN', ',',\n",
              "        'VBN', '-NONE-', 'IN', 'JJS', 'NNS', 'CC', 'JJ', 'NNS', ',', 'NNP',\n",
              "        'NNP', 'VBD', '-NONE-', '-NONE-', '.'], dtype='<U6'),\n",
              " array(['EX', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NNS', 'RB', '.', \"''\"],\n",
              "       dtype='<U4'),\n",
              " array(['JJR', 'IN', 'DT', 'JJ', 'NNS', 'VBP', '-NONE-', 'DT', 'JJ', 'NN',\n",
              "        'VBZ', 'PRP', 'TO', 'VB', 'TO', 'DT', 'NN', 'NN', 'IN', 'JJ',\n",
              "        'NNS', '.'], dtype='<U6'),\n",
              " array(['NNP', ',', '-NONE-', 'VBG', '-NONE-', 'TO', 'VB', 'NN', 'IN',\n",
              "        'JJ', 'NNP', 'NN', ',', 'VBD', 'JJ', 'NN', 'NNS', 'IN', 'CD', 'CC',\n",
              "        'VBD', '-NONE-', 'PRP', 'MD', 'VB', 'DT', 'JJ', 'NN', 'NN', 'IN',\n",
              "        'NNS', '.'], dtype='<U6'),\n",
              " array(['DT', 'NNP', 'VBZ', 'CD', 'IN', 'DT', 'JJ', 'VBN', 'NNS', 'WDT',\n",
              "        '-NONE-', 'VBZ', 'RB', 'VB', 'DT', 'JJR', 'NN', 'IN', 'NN', 'IN',\n",
              "        'DT', 'JJ', ',', 'JJ', 'NNS', 'JJ', 'IN', 'NN', 'WDT', '-NONE-',\n",
              "        'VBP', 'VBN', '-NONE-', 'IN', 'NNS', ',', 'VBG', 'TO', 'NNP',\n",
              "        'NNP', 'NNP', ',', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NNP', 'IN',\n",
              "        'NNP', 'NNP', 'IN', 'NNP', '.'], dtype='<U6')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map word to sentence in train and test\n",
        "# if it has a word change every word to lower case. if do not has word in dictionary set to oov\n",
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        "\n",
        "for s in train_sentences:\n",
        "  s_int = []\n",
        "  for w in s:\n",
        "    try:\n",
        "      s_int.append(word2index[w.lower()])\n",
        "    except KeyError:\n",
        "      s_int.append(word2index['-00V-'])\n",
        "  train_sentences_X.append(s_int)\n",
        "\n",
        "  for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "      try:\n",
        "        s_int.append(word2index[w.lower()])\n",
        "      except KeyError:\n",
        "        s_int.append(word2index['-OOV-'])\n",
        "  test_sentences_X.append(s_int)\n",
        "\n",
        "  for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "  for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        "print(len(train_sentences_X),len(test_sentences_X),len(train_tags_y),len(test_tags_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH1Yg-2llDJ1",
        "outputId": "85e947e0-8097-44e9-ddbf-54917b0dabd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[748, 321, 567, 438, 434, 524, 461, 499, 80, 217, 78, 151, 416, 136, 205]\n",
            "[78, 688, 321, 30, 370, 78, 293, 33, 1, 266, 1, 130, 695, 94, 113, 239, 1, 370, 1, 699, 78, 1, 528, 1, 457, 1, 398, 334, 266, 667, 329, 1, 1, 398, 1, 528, 525, 488, 1, 1, 380, 528, 113, 1, 370, 1, 714, 78, 1, 370, 1, 1, 370, 792, 205]\n",
            "[19, 6, 33, 8, 8, 14, 17, 34, 1, 34, 33, 8, 17, 17, 22]\n",
            "[34, 5, 1, 34, 33, 17, 13, 10, 7, 17, 34, 33, 19, 1, 7, 12, 10, 23, 17, 34, 33, 5, 22]\n",
            "80 80 6400 1600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how many word in our longest sentence\n",
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lznd0QjtnrKi",
        "outputId": "49181354-8089-4ceb-cf40-d6a4e688c475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# every sentences have same length the longest sentences because add padding(0) for same length\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding = 'post')\n",
        "\n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        "print(len(train_sentences_X),len(test_sentences_X),len(train_tags_y),len(test_tags_y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Snk3Rw9oADe",
        "outputId": "af3b53b6-361e-47e1-9357-947e767f3b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[748 321 567 438 434 524 461 499  80 217  78 151 416 136 205   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0]\n",
            "[  1 266   1 130 695  94 113 239   1 370   1 699  78   1 528   1 457   1\n",
            " 398 334 266 667 329   1   1 398   1 528 525 488   1   1 380 528 113   1\n",
            " 370   1 714  78   1 370   1   1 370 792 205]\n",
            "[19  6 33  8  8 14 17 34  1 34 33  8 17 17 22  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[34  5  1 34 33 17 13 10  7 17 34 33 19  1  7 12 10 23 17 34 33  5 22  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "80 80 6400 1600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH)))\n",
        "model.add(Embedding(len(word2index),128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZBUSmzpqtPY",
        "outputId": "c561a23a-a94d-40ac-9625-2da2a325699e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 47, 128)           102912    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 47, 512)          788480    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 47, 36)           18468     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 47, 36)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 909,860\n",
            "Trainable params: 909,860\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding in tag(label)\n",
        "def to_categorical(sequences, categories):\n",
        "  cat_sequences = []\n",
        "  for s in sequences:\n",
        "    cats = []\n",
        "    for item in s:\n",
        "      cats.append(np.zeros(categories))\n",
        "      cats[-1][item] = 1.0\n",
        "    cat_sequences.append(cats)\n",
        "  return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "2Y9THeoztLJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekApyX--xI4D",
        "outputId": "02ac0905-13ae-458b-b602-1bfe9cbfb815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px8rHyABxWpy",
        "outputId": "c0cfb52c-0eb3-4f17-9145-c2bb45b4339b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.6059 - accuracy: 0.0180 - val_loss: 3.5309 - val_accuracy: 0.4468\n",
            "Epoch 2/40\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3.5231 - accuracy: 0.5037 - val_loss: 3.4559 - val_accuracy: 0.4455\n",
            "Epoch 3/40\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 3.4367 - accuracy: 0.5010 - val_loss: 3.3618 - val_accuracy: 0.4455\n",
            "Epoch 4/40\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 3.3277 - accuracy: 0.5010 - val_loss: 3.2220 - val_accuracy: 0.4455\n",
            "Epoch 5/40\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 3.1644 - accuracy: 0.5010 - val_loss: 2.9806 - val_accuracy: 0.4455\n",
            "Epoch 6/40\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 2.8781 - accuracy: 0.5010 - val_loss: 2.5908 - val_accuracy: 0.4455\n",
            "Epoch 7/40\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 2.4135 - accuracy: 0.5010 - val_loss: 2.3784 - val_accuracy: 0.4455\n",
            "Epoch 8/40\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 2.1962 - accuracy: 0.5010 - val_loss: 2.2583 - val_accuracy: 0.4455\n",
            "Epoch 9/40\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.0209 - accuracy: 0.5010 - val_loss: 2.2654 - val_accuracy: 0.4455\n",
            "Epoch 10/40\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.9899 - accuracy: 0.5010 - val_loss: 2.2073 - val_accuracy: 0.4481\n",
            "Epoch 11/40\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.9368 - accuracy: 0.5037 - val_loss: 2.1282 - val_accuracy: 0.4521\n",
            "Epoch 12/40\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.8806 - accuracy: 0.5110 - val_loss: 2.0838 - val_accuracy: 0.4641\n",
            "Epoch 13/40\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.8677 - accuracy: 0.5186 - val_loss: 2.0755 - val_accuracy: 0.4734\n",
            "Epoch 14/40\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.8878 - accuracy: 0.5389 - val_loss: 2.0621 - val_accuracy: 0.4840\n",
            "Epoch 15/40\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.8767 - accuracy: 0.5489 - val_loss: 2.0410 - val_accuracy: 0.4894\n",
            "Epoch 16/40\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.8269 - accuracy: 0.5426 - val_loss: 2.0430 - val_accuracy: 0.4840\n",
            "Epoch 17/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.7705 - accuracy: 0.5322 - val_loss: 2.0782 - val_accuracy: 0.4840\n",
            "Epoch 18/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.7546 - accuracy: 0.5286 - val_loss: 2.1026 - val_accuracy: 0.4880\n",
            "Epoch 19/40\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.7515 - accuracy: 0.5319 - val_loss: 2.0741 - val_accuracy: 0.4960\n",
            "Epoch 20/40\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.7092 - accuracy: 0.5406 - val_loss: 1.9986 - val_accuracy: 0.5040\n",
            "Epoch 21/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.6672 - accuracy: 0.5515 - val_loss: 1.9245 - val_accuracy: 0.5093\n",
            "Epoch 22/40\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.6380 - accuracy: 0.5555 - val_loss: 1.8750 - val_accuracy: 0.5080\n",
            "Epoch 23/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.6046 - accuracy: 0.5575 - val_loss: 1.8592 - val_accuracy: 0.5106\n",
            "Epoch 24/40\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.5824 - accuracy: 0.5598 - val_loss: 1.8651 - val_accuracy: 0.5120\n",
            "Epoch 25/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.5710 - accuracy: 0.5588 - val_loss: 1.8777 - val_accuracy: 0.5093\n",
            "Epoch 26/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.5641 - accuracy: 0.5582 - val_loss: 1.8872 - val_accuracy: 0.5093\n",
            "Epoch 27/40\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.5572 - accuracy: 0.5562 - val_loss: 1.8876 - val_accuracy: 0.5106\n",
            "Epoch 28/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.5475 - accuracy: 0.5562 - val_loss: 1.8785 - val_accuracy: 0.5106\n",
            "Epoch 29/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.5359 - accuracy: 0.5552 - val_loss: 1.8659 - val_accuracy: 0.5239\n",
            "Epoch 30/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.5259 - accuracy: 0.5801 - val_loss: 1.8577 - val_accuracy: 0.5199\n",
            "Epoch 31/40\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.5189 - accuracy: 0.5771 - val_loss: 1.8600 - val_accuracy: 0.4947\n",
            "Epoch 32/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.5127 - accuracy: 0.5559 - val_loss: 1.8760 - val_accuracy: 0.4854\n",
            "Epoch 33/40\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.5053 - accuracy: 0.5459 - val_loss: 1.9061 - val_accuracy: 0.4854\n",
            "Epoch 34/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.4972 - accuracy: 0.5452 - val_loss: 1.9451 - val_accuracy: 0.4827\n",
            "Epoch 35/40\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.4904 - accuracy: 0.5439 - val_loss: 1.9790 - val_accuracy: 0.4827\n",
            "Epoch 36/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.4851 - accuracy: 0.5439 - val_loss: 1.9926 - val_accuracy: 0.4814\n",
            "Epoch 37/40\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.4784 - accuracy: 0.5445 - val_loss: 1.9841 - val_accuracy: 0.4880\n",
            "Epoch 38/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.4703 - accuracy: 0.5578 - val_loss: 1.9681 - val_accuracy: 0.5133\n",
            "Epoch 39/40\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.4635 - accuracy: 0.5662 - val_loss: 1.9606 - val_accuracy: 0.5133\n",
            "Epoch 40/40\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.4579 - accuracy: 0.5608 - val_loss: 1.9713 - val_accuracy: 0.5120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5b8198430>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "# print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "metadata": {
        "id": "Pv8ledmox_NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lF779OTH-KVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # len(to_categorical(test_tags_y, len(tag2index)))\n",
        "# len(tag2index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bX4P9uv64bn",
        "outputId": "20780ad3-b3b3-41a6-a79b-25825859c41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me\".split(\" \"),\n",
        "    \"I was running every day for a month\".split(\" \")\n",
        "]\n",
        "print(test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPV-ty4yg-V",
        "outputId": "f8987d44-7eab-4994-b6ab-03e9b02d5f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['running', 'is', 'very', 'important', 'for', 'me'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "  s_int = []\n",
        "  for w in s:\n",
        "    try:\n",
        "      s_int.append(word2index[w.lower()])\n",
        "    except KeyError:\n",
        "      s_int.append(word2index['-OOV-'])\n",
        "  test_samples_X.append(s_int)\n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtbB9m7zywbI",
        "outputId": "b2318663-0025-4948-80bf-30bd593d771f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1 321 659   1 699   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  1 165   1   1 746 699 113   1   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2sLxGmZzwzX",
        "outputId": "71d2ab5d-88ba-4790-afaf-39308d044213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 690ms/step\n",
            "[[[5.89950522e-03 6.09556548e-02 3.73745593e-03 ... 8.66259709e-02\n",
            "   1.11601532e-01 4.15267050e-03]\n",
            "  [6.41434779e-03 6.17145039e-02 3.67763499e-03 ... 8.67674947e-02\n",
            "   1.11084647e-01 4.08719387e-03]\n",
            "  [6.87334500e-03 6.22265637e-02 3.67832067e-03 ... 8.52579549e-02\n",
            "   1.10751875e-01 4.04979428e-03]\n",
            "  ...\n",
            "  [9.98511732e-01 7.30851070e-06 1.27917389e-07 ... 2.13833414e-06\n",
            "   3.36732319e-06 3.03101686e-08]\n",
            "  [9.98366416e-01 8.22304628e-06 1.69725240e-07 ... 2.43918748e-06\n",
            "   3.82869894e-06 3.94911943e-08]\n",
            "  [9.98131931e-01 9.88189095e-06 2.33284240e-07 ... 2.98095370e-06\n",
            "   4.71358817e-06 5.36617755e-08]]\n",
            "\n",
            " [[5.54236863e-03 6.04543686e-02 3.77471955e-03 ... 8.71446505e-02\n",
            "   1.11965604e-01 4.17072186e-03]\n",
            "  [5.75029477e-03 6.10807426e-02 3.71698337e-03 ... 8.65530074e-02\n",
            "   1.12243481e-01 4.13642637e-03]\n",
            "  [6.11612340e-03 6.08664267e-02 3.67016811e-03 ... 8.63808021e-02\n",
            "   1.13005541e-01 4.04495141e-03]\n",
            "  ...\n",
            "  [9.98511851e-01 7.30794682e-06 1.27963531e-07 ... 2.13996213e-06\n",
            "   3.36916401e-06 3.03209831e-08]\n",
            "  [9.98366535e-01 8.22270249e-06 1.69801680e-07 ... 2.44074681e-06\n",
            "   3.83109500e-06 3.95015185e-08]\n",
            "  [9.98132050e-01 9.88266493e-06 2.33375516e-07 ... 2.98225655e-06\n",
            "   4.71606654e-06 5.36680247e-08]]] (2, 47, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_tokens(sequences,index):\n",
        "  token_sequences = []\n",
        "  for categorical_sequence in sequences:\n",
        "    token_sequence = []\n",
        "    for categorical in categorical_sequence:\n",
        "      token_sequence.append(index[np.argmax(categorical)])\n",
        "    token_sequences.append(token_sequences)"
      ],
      "metadata": {
        "id": "SinWaUhW0G9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-0elKcb1C_v",
        "outputId": "6500c52f-79c5-4c46-c838-b6823736dc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['JJR', 'IN', 'DT', 'JJ', 'NNS', 'VBP', '-NONE-', 'DT', 'JJ', 'NN', 'VBZ', 'PRP', 'TO', 'VB', 'TO', 'DT', 'NN', 'NN', 'IN', 'JJ', 'NNS', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ],
      "metadata": {
        "id": "s-BKwq5I5IQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "jlxqogfR2m-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b539af2e-fd6d-44fb-aa4d-d6b4c2555e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I like you'\n",
        "token = nltk.word_tokenize(sentence)\n",
        "token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nUaspk354We",
        "outputId": "477c9ed0-2e6b-4c7d-813a-ad99f85ffd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'like', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ub0ReDL6G01",
        "outputId": "7e4b2b42-ab95-4a5d-e75e-2f9feb0aa578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('like', 'VBP'), ('you', 'PRP')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyThaiNLP"
      ],
      "metadata": {
        "id": "qY2I2IMJ6c96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp[full]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8-VsKyh6MnU",
        "outputId": "bb447910-539e-4a20-8485-d22863a2a601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pythainlp[full]\n",
            "  Downloading pythainlp-4.0.2-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (2.27.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (6.0)\n",
            "Collecting attacut>=1.0.4 (from pythainlp[full])\n",
            "  Downloading attacut-1.0.6-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji>=0.5.1 (from pythainlp[full])\n",
            "  Downloading emoji-2.4.0.tar.gz (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting epitran>=1.1 (from pythainlp[full])\n",
            "  Downloading epitran-1.24-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairseq>=0.10.0 (from pythainlp[full])\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (4.3.1)\n",
            "Requirement already satisfied: nltk>=3.3 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (1.5.3)\n",
            "Collecting pyicu>=2.3 (from pythainlp[full])\n",
            "  Downloading PyICU-2.11.tar.gz (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.9/257.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses>=0.0.41 (from pythainlp[full])\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece>=0.1.91 (from pythainlp[full])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ssg>=0.0.8 (from pythainlp[full])\n",
            "  Downloading ssg-0.0.8-py3-none-any.whl (473 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp[full]) (2.0.1+cu118)\n",
            "Collecting fastai<2.0 (from pythainlp[full])\n",
            "  Downloading fastai-1.0.61-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from pythainlp[full])\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting transformers>=4.22.1 (from pythainlp[full])\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sefr-cut>=1.1 (from pythainlp[full])\n",
            "  Downloading SEFR_CUT-1.1-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting phunspell>=0.1.6 (from pythainlp[full])\n",
            "  Downloading phunspell-0.1.6.tar.gz (47.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spylls>=0.1.5 (from pythainlp[full])\n",
            "  Downloading spylls-0.1.7-py2.py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting symspellpy>=6.7.6 (from pythainlp[full])\n",
            "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oskut>=1.3 (from pythainlp[full])\n",
            "  Downloading OSKut-1.3-py3-none-any.whl (44.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpo3>=1.2.2 (from pythainlp[full])\n",
            "  Downloading nlpo3-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.10.0 (from pythainlp[full])\n",
            "  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thai-nner (from pythainlp[full])\n",
            "  Downloading thai_nner-0.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wunsen>=0.0.3 (from pythainlp[full])\n",
            "  Downloading wunsen-0.0.3-py3-none-any.whl (21 kB)\n",
            "Collecting spacy-thai>=0.7.1 (from pythainlp[full])\n",
            "  Downloading spacy_thai-0.7.3-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ufal.chu-liu-edmonds>=1.0.2 (from pythainlp[full])\n",
            "  Downloading ufal.chu_liu_edmonds-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from attacut>=1.0.4->pythainlp[full])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire>=0.1.3 (from attacut>=1.0.4->pythainlp[full])\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nptyping>=0.2.0 (from attacut>=1.0.4->pythainlp[full])\n",
            "  Downloading nptyping-2.5.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from attacut>=1.0.4->pythainlp[full]) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->pythainlp[full]) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran>=1.1->pythainlp[full]) (67.7.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran>=1.1->pythainlp[full]) (2022.10.31)\n",
            "Collecting panphon>=0.20 (from epitran>=1.1->pythainlp[full])\n",
            "  Downloading panphon-0.20.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marisa-trie (from epitran>=1.1->pythainlp[full])\n",
            "  Downloading marisa_trie-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.10.0->pythainlp[full]) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.10.0->pythainlp[full]) (0.29.34)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitarray (from fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading bitarray-2.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.7/273.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.10.0->pythainlp[full]) (2.0.2+cu118)\n",
            "Collecting bottleneck (from fastai<2.0->pythainlp[full])\n",
            "  Downloading Bottleneck-1.3.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.0/354.0 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (1.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (4.11.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (3.7.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (2.8.4)\n",
            "Collecting nvidia-ml-py3 (from fastai<2.0->pythainlp[full])\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (8.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (1.10.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from fastai<2.0->pythainlp[full]) (0.15.2+cu118)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->pythainlp[full]) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.3->pythainlp[full]) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.3->pythainlp[full]) (1.2.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.10.0->pythainlp[full])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.10.0->pythainlp[full]) (23.3.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.10.0->pythainlp[full]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.10.0->pythainlp[full]) (1.11.1)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from oskut>=1.3->pythainlp[full]) (2.12.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from oskut>=1.3->pythainlp[full]) (1.2.2)\n",
            "Collecting pyahocorasick<=1.4.0 (from oskut>=1.3->pythainlp[full])\n",
            "  Downloading pyahocorasick-1.4.0.tar.gz (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->pythainlp[full]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->pythainlp[full]) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp[full]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp[full]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp[full]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp[full]) (3.4)\n",
            "Collecting python-crfsuite (from sefr-cut>=1.1->pythainlp[full])\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deplacy>=2.0.3 (from spacy-thai>=0.7.1->pythainlp[full])\n",
            "  Downloading deplacy-2.0.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy-thai>=0.7.1->pythainlp[full]) (3.5.2)\n",
            "Collecting ufal.udpipe>=1.2.0 (from spacy-thai>=0.7.1->pythainlp[full])\n",
            "  Downloading ufal.udpipe-1.3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (936 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m937.0/937.0 kB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting editdistpy>=0.1.3 (from symspellpy>=6.7.6->pythainlp[full])\n",
            "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pythainlp[full]) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pythainlp[full]) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pythainlp[full]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pythainlp[full]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pythainlp[full]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pythainlp[full]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pythainlp[full]) (16.0.5)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.22.1->pythainlp[full])\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.22.1->pythainlp[full])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting khanaa>=0.0.6 (from wunsen>=0.0.3->pythainlp[full])\n",
            "  Downloading khanaa-0.0.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.10/dist-packages (from thai-nner->pythainlp[full]) (2.12.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.1.3->attacut>=1.0.4->pythainlp[full]) (2.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.22.1->pythainlp[full]) (2023.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran>=1.1->pythainlp[full])\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran>=1.1->pythainlp[full]) (0.6.2)\n",
            "Collecting munkres (from panphon>=0.20->epitran>=1.1->pythainlp[full])\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq>=0.10.0->pythainlp[full]) (0.8.10)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq>=0.10.0->pythainlp[full])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq>=0.10.0->pythainlp[full]) (4.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (1.10.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->thai-nner->pythainlp[full]) (0.40.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (0.32.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fastai<2.0->pythainlp[full]) (2.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq>=0.10.0->pythainlp[full]) (2.21)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.10.0->pythainlp[full])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pythainlp[full]) (2.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai<2.0->pythainlp[full]) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai<2.0->pythainlp[full]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai<2.0->pythainlp[full]) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai<2.0->pythainlp[full]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai<2.0->pythainlp[full]) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->oskut>=1.3->pythainlp[full]) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.10.0->pythainlp[full]) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->thai-nner->pythainlp[full]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->thai-nner->pythainlp[full]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->thai-nner->pythainlp[full]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=1.14->thai-nner->pythainlp[full]) (1.3.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.0.0->oskut>=1.3->pythainlp[full]) (0.1.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.2.2->spacy-thai>=0.7.1->pythainlp[full]) (0.0.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->thai-nner->pythainlp[full]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=1.14->thai-nner->pythainlp[full]) (3.2.2)\n",
            "Building wheels for collected packages: emoji, fairseq, phunspell, pyicu, sacremoses, docopt, editdistpy, fire, antlr4-python3-runtime, pyahocorasick, nvidia-ml-py3, unicodecsv\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.4.0-py2.py3-none-any.whl size=350809 sha256=18cd2ba221558cd2915d0d4d135bd83c7393e022837157384d67d4d9d365c085\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/29/1c/234cae4632803c2ba4a76a71a679eb1383cf590775714e2a21\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11170781 sha256=e8032e4703627293f68b29772d4554bdc0111ab9a4371b5c2ba9451a3ffc54c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for phunspell (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phunspell: filename=phunspell-0.1.6-py3-none-any.whl size=47556195 sha256=3d8ddb1178c4ff79f5cbabaa060ff7363353b2fa0551080abbce8ad3d0bf8e53\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/4b/8b/24576e5451edf7f3f6b15492ace089f4f4353e6289fb923a7d\n",
            "  Building wheel for pyicu (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyicu: filename=PyICU-2.11-cp310-cp310-linux_x86_64.whl size=1855546 sha256=0d7c0c6390b73faac3f2428f7e8f4d432f5d93d592847d36a04af41432c79dc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/c3/00/2176cc05d3ea22935a9c78f1034b1a3e3697ef11ffb5cbe2f5\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=d1a200ea908fcb79eb5609fe8c82e69c5516b31997e4a7780e766fd12eecd15a\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=0e3cce60f4817d0e69d14d7001b9b43b7ec639489fd081268ec0f4d5bf4db265\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for editdistpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl size=141758 sha256=a1303d2b78758685ae0890d00646de601a20d5459faed96040b58ff853cb8686\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/6a/a6/a1283cc145323a1fb3d475bd158ee60b248ab1985230d266fc\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=3b23a7a20edf23b29f2136119a52e46e99bb499eff7b89aa5aa5f40cf262346a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=8adf180d14851f943afcbbcfb37c696ff1b156714e04569e2410862f49ecc58c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp310-cp310-linux_x86_64.whl size=94087 sha256=e1109c77d080518384399b49918373e3d9a6da4a22539fdaa4456645f5d366f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/2c/59/a06c8cb22b93704fbb6c96a4bd68e71b43bb7a4bc2f0410d81\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=02a3a674e8a9d34084daea627c3f6499e83762f56d13b4bc148a18f766d49dd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=3ace947e9315d826ffa5ef70848c795de41fe6a695286ae77dd09965f0677b67\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built emoji fairseq phunspell pyicu sacremoses docopt editdistpy fire antlr4-python3-runtime pyahocorasick nvidia-ml-py3 unicodecsv\n",
            "Installing collected packages: unicodecsv, ufal.udpipe, ufal.chu-liu-edmonds, tokenizers, sentencepiece, python-crfsuite, pyicu, pyahocorasick, nvidia-ml-py3, munkres, docopt, bitarray, antlr4-python3-runtime, spylls, sacremoses, portalocker, panphon, omegaconf, nptyping, nlpo3, marisa-trie, khanaa, humanfriendly, fire, emoji, editdistpy, deplacy, colorama, bottleneck, wunsen, symspellpy, ssg, sacrebleu, pythainlp, phunspell, hydra-core, huggingface-hub, epitran, coloredlogs, transformers, onnxruntime, bpemb, spacy-thai, sefr-cut, oskut, thai-nner, fastai, fairseq, attacut\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.12\n",
            "    Uninstalling fastai-2.7.12:\n",
            "      Successfully uninstalled fastai-2.7.12\n",
            "Successfully installed antlr4-python3-runtime-4.8 attacut-1.0.6 bitarray-2.7.4 bottleneck-1.3.7 bpemb-0.3.4 colorama-0.4.6 coloredlogs-15.0.1 deplacy-2.0.5 docopt-0.6.2 editdistpy-0.1.3 emoji-2.4.0 epitran-1.24 fairseq-0.12.2 fastai-1.0.61 fire-0.5.0 huggingface-hub-0.15.1 humanfriendly-10.0 hydra-core-1.0.7 khanaa-0.0.6 marisa-trie-0.8.0 munkres-1.1.4 nlpo3-1.3.0 nptyping-2.5.0 nvidia-ml-py3-7.352.0 omegaconf-2.0.6 onnxruntime-1.15.0 oskut-1.3 panphon-0.20.0 phunspell-0.1.6 portalocker-2.7.0 pyahocorasick-1.4.0 pyicu-2.11 pythainlp-4.0.2 python-crfsuite-0.9.9 sacrebleu-2.3.1 sacremoses-0.0.53 sefr-cut-1.1 sentencepiece-0.1.99 spacy-thai-0.7.3 spylls-0.1.7 ssg-0.0.8 symspellpy-6.7.7 thai-nner-0.3 tokenizers-0.13.3 transformers-4.29.2 ufal.chu-liu-edmonds-1.0.2 ufal.udpipe-1.3.0.1 unicodecsv-0.14.1 wunsen-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "from pythainlp import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "1gv7suh963WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"หัวหน้าฮงเป็นตัวอย่างของคนที่ดีเกินไปจนทำร้ายตัวเอง\""
      ],
      "metadata": {
        "id": "-u0ycVAW7CQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = word_tokenize(text, engine='newmm')\n",
        "print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaLnatwl7LJH",
        "outputId": "555fa30f-55ad-4027-a11b-453647d380e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['หัวหน้า', 'ฮง', 'เป็น', 'ตัวอย่าง', 'ของ', 'คน', 'ที่', 'ดี', 'เกินไป', 'จน', 'ทำร้าย', 'ตัวเอง']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import Tokenizer\n",
        "from pythainlp.util import Trie\n",
        "from pythainlp.corpus.common import thai_words\n",
        "words = [\"ดีเกินไป\"]\n",
        "custom_words_list = set(thai_words())\n",
        "custom_words_list.update(words)\n",
        "trie = Trie(words=custom_words_list)\n",
        "custom_tokenizer = Tokenizer(custom_dict=trie, engine='newmm')\n",
        "sent = custom_tokenizer.word_tokenize(text)\n",
        "print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yFr5iRO7R_f",
        "outputId": "6c6906f4-8500-4269-9bad-4438702a0d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['หัวหน้า', 'ฮง', 'เป็น', 'ตัวอย่าง', 'ของ', 'คน', 'ที่', 'ดีเกินไป', 'จน', 'ทำร้าย', 'ตัวเอง']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tag import pos_tag, pos_tag_sents\n",
        "pos_tag(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71boGtsU8ZTj",
        "outputId": "b024f572-2345-41ae-f04f-47ff7af5cb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NCMN'),\n",
              " ('ฮง', 'NCMN'),\n",
              " ('เป็น', 'VSTA'),\n",
              " ('ตัวอย่าง', 'NCMN'),\n",
              " ('ของ', 'RPRE'),\n",
              " ('คน', 'NCMN'),\n",
              " ('ที่', 'PREL'),\n",
              " ('ดีเกินไป', 'VSTA'),\n",
              " ('จน', 'JSBR'),\n",
              " ('ทำร้าย', 'DONM'),\n",
              " ('ตัวเอง', 'PDMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(sent, corpus=\"pud\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnwBv9Cf9AnN",
        "outputId": "a4ba72c6-c987-41b7-d62a-498eb77d6fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NOUN'),\n",
              " ('ฮง', 'PROPN'),\n",
              " ('เป็น', 'AUX'),\n",
              " ('ตัวอย่าง', 'NOUN'),\n",
              " ('ของ', 'ADP'),\n",
              " ('คน', 'NOUN'),\n",
              " ('ที่', 'DET'),\n",
              " ('ดีเกินไป', 'VERB'),\n",
              " ('จน', 'ADP'),\n",
              " ('ทำร้าย', 'ADP'),\n",
              " ('ตัวเอง', 'PRON')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(sent, engine=\"unigram\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eS9Sadg9GGz",
        "outputId": "74bcd95d-ff01-4ad4-89f8-36ce19b3c7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NCMN'),\n",
              " ('ฮง', ''),\n",
              " ('เป็น', 'VSTA'),\n",
              " ('ตัวอย่าง', 'NCMN'),\n",
              " ('ของ', 'RPRE'),\n",
              " ('คน', 'CNIT'),\n",
              " ('ที่', 'PREL'),\n",
              " ('ดีเกินไป', ''),\n",
              " ('จน', 'JSBR'),\n",
              " ('ทำร้าย', ''),\n",
              " ('ตัวเอง', 'PDMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGK9EOL_9Vr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}