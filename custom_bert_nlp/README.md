# Project
custom Bert NLP for work on another task.

# Problem
adapting the pre-trained BERT model to the specific task at hand. BERT (Bidirectional Encoder Representations from Transformers) is a powerful language representation model pre-trained on large amounts of text data. However, it requires further modifications and fine-tuning to be effectively used for different NLP tasks. Here are the key challenges and solutions when customizing BERT for other tasks

# Solution
we create custom Bert NLP for work in another task.

# Methodology
1. Install Library
2. Import Dataset
3. download p-train thai keras
4. cut Thai word
5. pre-train checkpoint and cut the word
6. cut sub word with sentence-piece
7. Fine-tuning
8. save the output for use in another task